# Large model configuration for reference implementation
# Requires powerful GPU (24GB+ VRAM recommended)

model:
  vocab_size: 32000
  d_model: 768
  n_heads: 12
  n_layers: 12
  d_ff: 3072
  max_seq_len: 2048
  dropout: 0.1
  layer_norm_eps: 1.0e-6
  tie_weights: true

training:
  batch_size: 8
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 1000
  max_steps: 100000
  gradient_clip: 1.0
  log_interval: 100
  eval_interval: 2000
  save_interval: 10000

data:
  seq_length: 2048
  train_split: 0.98
  num_workers: 8
  pin_memory: true
